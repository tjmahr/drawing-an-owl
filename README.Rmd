---
title: "Untitled"

bibliography: refs.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

@JSSv078i02 describes the statistical machinery for their latent class mixed models. We note the following equations in their paper.

Below is the likelihood contribution for one individual in the basic linear
mixed model that we know and love:

$$
L_i = \phi_i(Y_i; \theta_1) \\
\phi: \textrm{MVN density} \\
i: \textrm{individuals}
$$

When there are $G$ latent classes, the likelihood becomes a weighted sum of
class-specific likelihoods:

$$
L_i(\theta_G) = \sum_{g=1}^G \pi_{ig}\phi_{ig}(Y_i|c_i = g; \theta_G) \\
\pi_{ig}: \textrm{probability of group membership for an individual} \\
g: \textrm{groups}
$$
So, an individual makes $G$ contributions to the likelihood and each one is
weighted by their group membership probability. And when there is one group,
this equation reduces to the first likelihood equation.

Finally, group probabilities are defined as a multinomial logistic model:

$$
\pi_{ig} = P(c_i = g | X_{ci}) = \frac{{}}{}
$$
