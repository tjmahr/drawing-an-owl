---
title: "Latent class mixed effects models"
bibliography: refs.bib
output: 
  github_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE, 
  dpi = 300,
  out.width = "40%"
)
library(tidyverse)
library(patchwork)

knitr::knit_engines$set(
  blockquote = function(options) {
    options$code <- c("<blockquote>", options$code, "</blockquote>")
    knitr::knit_engines$get("block2")(options)
  }
)
if (!is.null(knitr::pandoc_to())) {
  set.seed(101)
}
```


This document/repository is an exercise in me "drawing the owl", a
phrase used by Richard McElreath in the [Statistical
Rethinking](https://www.youtube.com/watch?v=FdnMWdICdRs&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus)
course to describe the process of the model development. This statistical workflow involves
simulating data, creating statistical models to infer the unobserved
parameters that generated the simulate data, and gradually building up
the model's complexity. And we do this model development *before* we 
plug the real data into the model.

## Background

In @mahr2020, we examined the developmental trajectories of speech
intelligibility in three groups of children with cerebral palsy: those
without speech-motor impairment (NSMI), those with speech-motor
impairment and typical language comprehension (SMI-LCT), and those with
speech-motor impairment and impaired language comprehension (SMI-LCI).
We made these groupings based on clinical judgment for NSMI/SMI status
and language testing for LCT/LCI status. We also tried to make the group
assignments based on age-4 data whenever possible to look at how the
prospective/predictive value of the groups on later growth trajectories.

```{r, out.width = "50%", echo = FALSE}
knitr::include_graphics("previous-paper.png")
```

The NSMI grouping is very successful: All of the children without
dysarthria seem following a homogeneous set of trajectories. The SMI-LCT
grouping seems to two have sets of trajectories: 1) trajectories that
are more spread out than the NSMI group but do show reliable growth in
intelligibility, and 2) 4--5 trajectories that show very limited growth.
The SMI-LCI group is less numerous than the other groups but it seems to
have a lower average trajectories than the others. 

So, these groupings provide a coarse ordering for severity of
impairment, but can we do better? Actually, that's not the right
question: Can we do different? What if instead we tell the statistical
model that there are K latent subgroups in our sample? Can it identify K
different groups? Is there evidence in the data for K groups? What about
K-1 or K+1 groups? Do the model-uncovered groups match up with our
previously defined groups? Those are just a few of the questions that 
spring to mind. 

I would like to apply a latent class mixed models approach to our 
intelligibility trajectories but there is no off-the-shelf solution for 
this problem, at least not one that can accommodate the nonlinear growth 
model and beta distribution family I used in my prior analysis. 

## Math of latent class mixed models

The [lcmm](https://cecileproust-lima.github.io/lcmm/) R package provides
a maximum-likelihood estimate for gaussian latent class mixed models, so
that is a good starting point for understanding these models.

@JSSv078i02 describe the statistical machinery for their latent class
mixed models. We note the following equations from their paper. Below is the 
likelihood contribution for one individual (group) in the basic
linear mixed model that we know and love:

$$
\displaylines{
L_i = \phi_i(Y_i; \theta_1) \\
\phi: \textrm{MVN density} \\
i: \textrm{individual index}
}
$$

When there are $G$ latent classes, the likelihood becomes a weighted sum
of class-specific likelihoods:

$$
\displaylines{
L_i(\theta_G) = \sum_{g=1}^G \pi_{ig}\phi_{ig}(Y_i|c_i = g; \theta_G) \\
\pi_{ig}: \textrm{probability of group membership for an individual} \\
g: \textrm{group index}
}
$$


So, an individual makes $G$ contributions to the likelihood and each one
is weighted by their group membership probability. And when there is one
group, this equation reduces to the first likelihood equation.

Finally, group probabilities are defined as a multinomial logistic
model:

$$
\displaylines{
\pi_{ig} = 
  P(c_i = g | X_{ci}) = 
  \frac{
    e^{\xi_{0g} +X^{\top}_{ci}\xi_{1g}}
  }{
    \sum^{G}_{l=1}e^{\xi_{0l} +X^{\top}_{ci}\xi_{1l}}
  } \\
c_i : \textrm{the latent class for an individual} \\
X_{ci} : \textrm{time-indpendent covariates} \\
\xi_{0g} +X^{\top}_{ci}\xi_{1g} : \textrm{linear model for group membership}
}
$$

This model is something to worry about until later.


## Marginalization of discrete parameters

<!-- Stan uses Hamiltonian Monte Carlo (HMC) to sample from the posterior -->
<!-- distribution. HMC is conceptually [a physics -->
<!-- simulation](https://mc-stan.org/docs/reference-manual/hamiltonian-monte-carlo.html) -->
<!-- where a sampling chain explores the probability surface and its -->
<!-- gradients like a skateboarder rolling around a skatepark. -->

Stan [does not support sampling latent discrete
parameters](https://mc-stan.org/docs/stan-users-guide/latent-discrete.html),
and the latent group memberships above are discrete parameters. But we
don't need them because we can average over them (or "marginalize" over
them). 

Richard McElreath has a
[tutorial](https://elevanth.org/blog/2018/01/29/algebra-and-missingness/)
where observations are in different states but for some observations,
that state is missing/unobserved. He provides following recipe for how 
work with discrete parameters in Stan:

```{blockquote}
(1) Write the probability of an outcome `y[i]` conditional on known values
of the discrete parameters. Call this $L$, the conditional likelihood.

(2) List all the possible states the discrete parameters could take. For
example, if you have two binary parameters, then there are four possible
states: 11, 10, 01, and 00. Let $j$ be an index for state, so that in
this example $j$ can take the values 1, 2, 3, and 4.

(3) For each state in (2), compute the probability of that state. Your
model provides these probabilities, and they will depend upon the
details of your model. Call each state's probability $P_j$.

(4) For each state in (2), compute the probability of an outcome `y[i]`
when the discrete parameters take on those values. For example, there is
a different probability for each of 11, 10, 01, and 00. You can use the
expression from (1) and just insert the values of the parameters for
each state. Call each state's corresponding likelihood $L_j$.

(5) Now you can compute the unconditional probability of `y[i]` by
multiplying each $P_j$ by $L_j$. Then sum these products for all
states: $M=\sum_j P_j L_j$. This $M$ is the marginal likelihood, the
probability of `y[i]` averaging over the unknown values of the discrete
parameters.

In the actual code, we must do all of the above on the log-probability
scale, or otherwise numerical precision will be poor. So in practice
each $P_j L_j$ term is computed as a sum of log probabilities:
`term[j] = logP[j] + logL[j]`. And then we can compute $\log M$ as
`log_sum_exp(term).`
```

Ben Lambert [p.401--406] also describes the process more generally:

```{blockquote}
It is still possible to use [discrete parameters in Stan] by
marginalizing them out of the joint log density. This amounts to
summing the joint density over all possible values of the discrete
parameter $\theta$:

$$
p(\beta) = \sum_{i=1}^{k}p(\beta,\theta_i)
$$

However, we must do so on the log probability scale because this is
what Stan uses:

$$
\begin{align*}
\log p(\beta) &= \log \sum_{i=1}^{k} p(\beta, \theta_i) \\
&= \log \sum_{i=1}^{k}\exp (\log p(\beta, \theta_i)) \\
&= \texttt{log\\_sum\\_exp}(\log p(\beta, \theta_i))
\end{align*}
$$ 
```

The thing I want to highlight from this formulation is that the left
sides of the equations contain a marginal probability of a continuous
parameter $\beta$ with no $\theta$s---hence "marginalization". We sum
over all the values of the discrete parameter $\theta$ and no longer
rely on it.



## A Gaussian mixture model

Following that recipe and the notes from the lcmm package, I can do a simple 
latent class model in Stan:

```{r}
# knitr::opts_chunk$set(eval = FALSE)
```


```{r}
m <- cmdstanr::cmdstan_model("0.stan")
```

```{embed, file="0.stan"}
```

In the double for-loop in the model block, we perform the
marginalization recipe. We iterate through the observations and compute
weighted likelihoods for each group by multiplying the group probability
times the likelihood of the observation in that group.

Note also in the `parameters` block that the `mean_group` variable has 
the type `ordered`. This constraint helps prevent the group ordering 
from being randomly reordered on each posterior sample.

Let's try to replicate the [Gaussian Mixture Model
demo](https://www.pymc.io/projects/examples/en/latest/mixture_models/gaussian_mixture_model.html)
from PyMC. First, we simulate the data and run the Stan model.

```{r}
library(dplyr)
library(ggplot2)
library(patchwork)

# Simulate GMM data and bundle data-generating parameters in a table
gmm_setup <- function(n_groups, n_obs, means, sds, probs = NULL) {
  if (is.null(probs)) {
    probs <- rep(1 / n_groups, n_groups)
  }
  ids <- sample(1:n_groups, n_obs, replace = TRUE, prob = probs)
  y <- rnorm(n_obs, means[ids], sds[ids])

  true_values <- data.frame(
    parameter = c(
      sprintf("mean_group[%s]", 1:3),
      sprintf("probs[%s]", 1:3),
      sprintf("sigma_group[%s]", 1:3)
    ),
    ground_truth = c(means, probs, sds)
  )
  
  list(
    data_stan = list(
      n_obs = length(y),
      n_groups = n_groups,
      y = y
    ),
    data_truth = true_values
  )
}

gmm <- gmm_setup(
  n_groups = 3, 
  n_obs = 500, 
  means = c(-5, 0, 5), 
  sds = c(0.5, 2.0, 0.75)
)

posterior_gmm <- m$sample(gmm$data_stan, refresh = 0, parallel_chains = 4)
```

Now we can plot the data and model estimates.

```{r gmm1, fig.height = 3, fig.width = 8, out.width = "100%"}
tidy_gmm_draws <- function(x) {
  x$draws() |> 
    posterior::as_draws_df() |>
    tibble::as_tibble() |> 
    select(
      .draw, .chain,
      starts_with("mean_"), 
      starts_with("probs"), 
      starts_with("sigma")
    ) |> 
    tidyr::pivot_longer(
      cols = c(-.draw, -.chain), 
      names_pattern = "(.+)\\[(\\d+)\\]",
      names_to = c("family", "index")
    ) |> 
    mutate(parameter = sprintf("%s[%s]", family, index))
}

plot_gmm_results <- function(data_draws, data_stan) {
  p <- ggplot(tibble(x = data_stan$y)) + 
    aes(x = x) + 
    ggdist::geom_swarm(color = "black") +
    ggtitle("data") +
    xlab("observed value") +
    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())

  p2 <- ggplot(data_draws) + 
    aes(x = value, y = index) + 
    ggdist::stat_pointinterval() + 
    facet_wrap("family", scales = "free") +
    geom_point(
      aes(x = ground_truth, color = "ground truth"), 
      position = position_nudge(y = .2),
      color = "orangered"
    ) +
    ggtitle("model") +
    guides(color = "none")

  p + p2 + plot_layout(widths = c(1, 2))
}

data_draws <- posterior_gmm |> 
  tidy_gmm_draws()  |> 
  left_join(gmm$data_truth, by = "parameter")

plot_gmm_results(data_draws, gmm$data_stan)
```

There is another PyMC demo that sets the sampling probabilities of the
groups too. Here is where the functions we defined above save us some 
space.

```{r gmm2, fig.height = 3, fig.width = 8, out.width = "100%"}
gmm2 <- gmm_setup(
  n_groups = 3, 
  n_obs = 1000, 
  means = c(-5, 0, 5), 
  sds = c(0.5, 2.0, 0.75), 
  probs = c(0.35, 0.4, 0.25)
)

posterior_gmm2 <- m$sample(gmm2$data_stan, refresh = 0, parallel_chains = 4)

data_draws2 <- posterior_gmm2 |> 
  tidy_gmm_draws()  |> 
  left_join(gmm2$data_truth, by = "parameter")

plot_gmm_results(data_draws2, gmm2$data_stan)
```

## The gaussian mixed mixture model

Now let's try a gaussian mixed effects mixture model. We have
repeated-measures observations drawn from k latent groups. First, let's
validate the model in the $k=1$ case, which should reduce a random
intercept model.

```{r}
# knitr::opts_chunk$set(eval = FALSE)
```

```{r}
m <- cmdstanr::cmdstan_model("0-b.stan")
```

```{embed, file="0-b.stan"}
```

When I first fit this model, I didn't get a good fit at first. Then 
standardizing the observations fixed things. I suspect that the 
observation scale (hundreds of milliseconds) didn't work with my 
hard-coded priors (wide for standardized variables).

```{r}
# Borrow the sleepstudy dataset
d <- lme4::sleepstudy |> 
  mutate(
    y = Reaction |> scale() |> as.vector(),
    individual = match(Subject, unique(Subject))
  )

e <- m$sample(
  data = list(
    n_obs = nrow(d),
    n_groups = 1,
    n_individuals = length(unique(d$Subject)),
    individual = d$individual,
    y = d$y
  ), 
  parallel_chains = 4
)
# e$summary() |> print(n = Inf)
e_sum <- e$summary(
  variables = c("mean_group", "sigma_intercepts", "sigma_residuals")
)
e_sum
```

Compare with REML estimate. 

```{r}
lme4::lmer(y ~ 1 + (1 | Subject), d, REML = TRUE) |> 
  broom.mixed::tidy()
```





```{r gmmm1}
gmmm_setup <- function(
  n_groups, 
  n_obs, 
  n_individuals, 
  means, 
  sd_resid, 
  sd_group, 
  probs = NULL
) {
  if (is.null(probs)) {
    probs <- rep(1 / n_groups, n_groups)
  }
  
  make_unscaler <- function(y) {
    z <- scale(y)
    center_value <- attr(z, "scaled:center")
    scale_value <- attr(z, "scaled:scale")
    function(x, center = TRUE, scale = TRUE) {
      if (!center) center_value <- 0
      if (!scale) scale_value <- 1
      (x * scale_value) + center_value
    }
  }
  make_rescaler <- function(y) {
    z <- scale(y)
    center_value <- attr(z, "scaled:center")
    scale_value <- attr(z, "scaled:scale")
    function(x, center = TRUE, scale = TRUE) {
      if (!center) center_value <- 0
      if (!scale) scale_value <- 1
      (x - center_value) / scale_value
    }
  }

  data_groups <- tibble::tibble(
    group = seq_len(n_groups),
    probs = probs,
    mean_group = means,
    sd_group = sd_group,
    sd_resid = sd_resid
  )

  data_individuals <- tibble::tibble(
    individual = seq_len(n_individuals),
    group = sample(1:n_groups, n_individuals, replace = TRUE, prob = probs)
  ) |> 
    left_join(data_groups, by = "group") |> 
    mutate(
      offset_individual = rnorm(n_individuals) * sd_group,
      mean_individual = mean_group + offset_individual
    )

    data_observed <- tibble::tibble(
      individual = sample(1:n_individuals, n_obs, replace = TRUE)
    ) |> 
      left_join(data_individuals, by = "individual") |> 
      mutate(
        y_raw = rnorm(n_obs, 0, sd_resid) + mean_individual,
        y = scale(y_raw) |> as.vector()
      )
  
    
  # true_values <- data.frame(
  #   parameter = c(
  #     sprintf("mean_group[%s]", 1:3),
  #     sprintf("probs[%s]", 1:3),
  #     sprintf("sigma_group[%s]", 1:3)
  #   ),
  #   ground_truth = c(means, probs, sds)
  # )
  
  list(
    data_stan = list(
      n_obs = n_obs,
      n_groups = n_groups,
      n_individuals = n_individuals,
      individual = data_observed$individual,
      y = data_observed$y
    ),
    data_groups = data_groups,
    data_individuals = data_individuals,
    data_observed = data_observed
  )
}

gmmm <- gmmm_setup(
  n_groups = 3, 
  n_obs = 400, 
  n_individuals = 60, 
  means = c(-5, 10, 20), 
  sd_group = c(4, 2, 1), 
  sd_resid = c(1, 1, 1)
)

ggplot(gmmm$data_observed) + 
  aes(x = fct_reorder(as.factor(individual), y), y = y) + 
  geom_point() +
  guides(x = "none") +
  labs(x = "individual")
  # geom_point(aes(y = (mean_group - mean(y_raw)) / sd(y_raw))) +
  # geom_segment(
  #   aes(
  #     x = individual, 
  #     xend = individual, 
  #     y = (mean_group - mean(y_raw)) / sd(y_raw), 
  #     yend = (mean_individual - mean(y_raw)) / sd(y_raw)
  #   )
  # )

# gmmm$data_observed
```

When I try to fit this model, all hell breaks loose. I need to figure out 
how to constrain/identify it.



```{r}
posterior_gmmm <- m$sample(
  gmmm$data_stan, 
  refresh = 200, 
  parallel_chains = 4, 
  max_treedepth = 15
)

draws <- posterior_gmmm$draws() |> 
  posterior::as_draws_df()

bayesplot::mcmc_dens_chains(draws, vars(starts_with("mean_group")))
bayesplot::mcmc_dens_chains(draws, vars(starts_with("probs")))
bayesplot::mcmc_dens_chains(draws, vars(starts_with("sigma")))


# e <- posterior_gmmm$summary()
# e |> print(n = 20)
# 
# 
# e |> 
#   filter(
#     variable |> startsWith("mean_") | 
#     variable |> startsWith("sigma") |
#     variable |> startsWith("probs") 
#   ) |> 
#   ggplot() + 
#   aes(x = median) +
#   geom_segment(aes(y = variable, yend = variable, x = q5, xend = q95)) +
#   geom_point(aes(y = variable)) +
#   geom_point(
#     aes(
#       y = sprintf("mean_group[%s]", group),
#       x = rescale(mean_group)
#     ),
#     data = gmmm$data_groups,
#     color = "orangered"
#   ) +
#   geom_point(
#     aes(
#       y = sprintf("sigma_intercepts[%s]", group),
#       x = rescale(sd_group, center = FALSE)
#     ),
#     data = gmmm$data_groups,
#     color = "orangered"
#   ) +
#   geom_point(
#     aes(
#       y = sprintf("sigma_residuals[%s]", group),
#       x = rescale(sd_group, center = FALSE)
#     ),
#     data = gmmm$data_groups,
#     color = "orangered"
#   )

# m2 <- brm(
#   formula = y ~ 1 + (1|individual), 
#   data = gmmm$data_observed,
#   family = mixture(gaussian(), gaussian()), 
#   backend = "cmdstanr"
# )
```

